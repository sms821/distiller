# Scheduler for training / re-training a model using quantization aware training, with a linear, range-based quantizer
#
# The setting here is 4-bit weights and 8-bit activations. For vision models, this is usually applied to the entire model,
# without exceptions. Hence, this scheduler isn't model-specific as-is. It doesn't define any name-based overrides.
#
# At the moment this quantizer will:
#  * Quantize weights and biases for all convolution and FC layers
#  * Quantize all ReLU activations
#
# Here's an example run for fine-tuning the ResNet-18 model from torchvision:
#
# python compress_classifier.py -a resnet18 -p 50 -b 256 ~/datasets/imagenet --epochs 10 --compress=../quantization/quant_aware_train/quant_aware_train_linear_quant.yaml --pretrained -j 22 --lr 0.0001 --vs 0 --gpu 0
#
# (Note that the command line above is not using --deterministic, so results could vary a little bit)

quantizers:
  linear_quantizer:
    class: QuantAwareTrainRangeLinearQuantizer
    bits_activations: 8
    bits_weights: 4
    mode: 'ASYMMETRIC_UNSIGNED'  # Can try "SYMMETRIC" as well
    ema_decay: 0.999   # Decay value for exponential moving average tracking of activation ranges
    per_channel_wts: True

lr_schedulers:
  my_lr: 
  class: ReduceLROnPlateau
  factor: 0.5
  verbose: True

policies:
    - quantizer:
        instance_name: linear_quantizer
      # For now putting a large range here, which should cover both training from scratch or resuming from some
      # pre-trained checkpoint at some unknown epoch
      starting_epoch: 0
      ending_epoch: 300
      frequency: 1

    - lr_scheduler:
        instance_name: my_lr
      starting_epoch: 0
      ending_epoch: 300
      frequency: 1
